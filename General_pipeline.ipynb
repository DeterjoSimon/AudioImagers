{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal as sp\n",
    "import IPython\n",
    "from IPython.display import Audio, HTML\n",
    "from scipy.io import wavfile\n",
    "import math\n",
    "import cv2\n",
    "import subprocess\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)\n",
    "import librosa\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "from matplotlib import cm\n",
    "import moviepy\n",
    "from os.path import abspath\n",
    "from os.path import join as p_join\n",
    "\n",
    "data_fpath = p_join(abspath(''), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Here is where songs + images can be loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal function are get_info and get_gif, which can be changed for more interesting parameter searching and mappings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Song details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img, target_width=480):\n",
    "    height, width, layers = img.shape\n",
    "    scale = target_width / width\n",
    "    target_width = int(width * scale)\n",
    "    target_height = int(height * scale)\n",
    "    dim = (target_width, target_height)\n",
    "    return cv2.resize(img, dim, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_frames(frames, norm_functions):\n",
    "    return [\n",
    "        { key: (norm_functions[key](value)\n",
    "                if key in norm_functions else value)\n",
    "         for key, value in frame.items() }\n",
    "        for frame in frames\n",
    "    ]\n",
    "\n",
    "# function that takes audio as input and return different features\n",
    "def get_info(audio_name, window_size):\n",
    "    # audio_name -- name of audiofile\n",
    "    # window_size -- we split audio into frames, this is the length of the frame in seconds\n",
    "    x, sr = librosa.load(audio_name, sr = None)\n",
    "    seconds = len(x) / sr\n",
    "    frame_len = int(len(x) / (seconds / window_size))\n",
    "    frames = []\n",
    "    for ind in range(0, len(x) + 1, frame_len):\n",
    "        current_amplitude = x[ind:ind + frame_len]\n",
    "        fft = librosa.stft(current_amplitude, n_fft=len(current_amplitude), hop_length=len(current_amplitude))\n",
    "        freqs = fft[:, 0]\n",
    "        freqs = librosa.amplitude_to_db(abs(freqs))\n",
    "        _, phase = librosa.magphase(fft)\n",
    "        phase = phase[:, 0]\n",
    "        frames.append({\n",
    "            'amplitude':current_amplitude,\n",
    "            'amplitude_abs_mean': np.mean(np.abs(current_amplitude)),\n",
    "            'frequency':freqs,\n",
    "            'phase':phase,\n",
    "        })\n",
    "        #timbre = 0\n",
    "    print(len(frames))\n",
    "    print(librosa.stft(x, n_fft=frame_len, hop_length = frame_len // 4))\n",
    "    max_amplitude_mean = max(frame['amplitude_abs_mean'] for frame in frames)\n",
    "    return normalize_frames(frames, {\n",
    "        'amplitude_abs_mean': lambda x: x / max_amplitude_mean\n",
    "    })\n",
    "\n",
    "def get_features(audio, sr, window_size_s):\n",
    "    x = audio\n",
    "    features = {}\n",
    "    frame_len = int(sr * window_size_s)\n",
    "    hop_length = frame_len // 4\n",
    "    # number of samples\n",
    "    n_frames = int(math.ceil(len(x) / hop_length))\n",
    "    \n",
    "    # stft\n",
    "    stft = librosa.stft(x, n_fft=frame_len, hop_length = hop_length)\n",
    "    #print(stft.shape, librosa.fft_frequencies(sr=sr, n_fft=frame_len).shape)\n",
    "    features['stft'] = stft.T\n",
    "    features['stft_db'] = librosa.amplitude_to_db(np.abs(stft)).T\n",
    "    _, phase = librosa.magphase(stft)\n",
    "    features['stft_phase'] = phase.T\n",
    "    \n",
    "    features['amplitude_abs_mean'] = []\n",
    "    features['values'] = []\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        window = x[max(0, i*hop_length - frame_len//2) : i*hop_length + frame_len//2]\n",
    "        features['values'].append(window)\n",
    "        features['amplitude_abs_mean'].append(np.mean(np.abs(window)))\n",
    "        \n",
    "    features['amplitude_abs_mean'] /= np.max(np.abs(features['amplitude_abs_mean']))\n",
    "\n",
    "    \n",
    "    # convert dict of lists to list of dicts\n",
    "    return [dict(zip(features,t), \n",
    "                param_n_fft=frame_len\n",
    "                ) for t in zip(*features.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(stft_sample_first, num_bins):\n",
    "    bin_size = int(len(stft_sample_first)/num_bins)\n",
    "    # bin the frequencies into three channels (low, middle, high frequencies) and compute average amplitude for each bin\n",
    "    binned_freq = np.array([np.mean(stft_sample_first[i*bin_size : (i+1)*bin_size]) for i in range(num_bins)])\n",
    "    return binned_freq\n",
    "\n",
    "def get_bounds(initial_rgb, initial_ampl, max_ampl, color_range):\n",
    "    lower = initial_rgb - ((initial_ampl/max_ampl) * color_range)\n",
    "    upper = initial_rgb + ( (1-(initial_ampl/max_ampl)) * color_range)\n",
    "    return lower, upper\n",
    "\n",
    "def get_new_rgb_from_range(lower, upper, max_ampl, ampl):\n",
    "    return (ampl/max_ampl) * (upper-lower) + lower\n",
    "\n",
    "def sound_to_image(img, features, features_initial, sr, mode=\"maximum\", window_size=100):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "    maximum = map the whole amplitude range to the whole color range\n",
    "    window = map the whole amplitude range to a color range of size window_size\n",
    "    \"\"\"\n",
    "    mean_rgb = np.array([np.mean(img[:,:,i]) for i in range(3)])\n",
    "    r = [mean_rgb[0]]\n",
    "    g = [mean_rgb[1]]\n",
    "    b = [mean_rgb[2]]\n",
    "\n",
    "    # first sample bin frequencies and compute mean amplitude for each frequency\n",
    "    stft_sample_first = np.abs(features_initial[\"stft\"])\n",
    "    num_bins = 3\n",
    "    binned_freq_init = get_bins(stft_sample_first, num_bins)\n",
    "    \n",
    "    # find maximum amplitude of first sample to get upper bound for the mapping\n",
    "    max_ampl = (255 * binned_freq_init) / mean_rgb\n",
    "    # window\n",
    "    if mode==\"window\":\n",
    "        lower, upper = get_bounds(mean_rgb, binned_freq_init, max_ampl, window_size)\n",
    "    else:\n",
    "        lower = 0\n",
    "        upper = 0\n",
    "\n",
    "    # modify image according to incoming new sample\n",
    "    stft_sample = np.abs(features[\"stft\"])\n",
    "\n",
    "    binned_freq_curr = get_bins(stft_sample, num_bins)\n",
    "    # modify image accordingly\n",
    "    # compute RGB values of previous image\n",
    "    # prev_values = np.array([np.mean(im_prev[:,:,i]) for i in range(3)])\n",
    "    # change according to change in frequency \n",
    "    # ratio of frequencies = ratio of colors\n",
    "    if mode == \"maximum\":\n",
    "        # ration of frequency to max frequency = ratio of color to max color\n",
    "        new_values = (255 * binned_freq_curr) / max_ampl\n",
    "    # get new values in given range (not from 0 to 255 but lower to upper which is a span of \"color_range\")\n",
    "    else:\n",
    "        new_values = get_new_rgb_from_range(lower, upper, max_ampl, binned_freq_curr)\n",
    "    #new_values = ((prev_values) * binned_freq_curr)/(binned_freq_prev) # (255 * binned_freq_curr) / max_ampl #(prev_values * binned_freq_curr)/binned_freq_prev\n",
    "    #print(new_values)\n",
    "    \n",
    "    im_new = img.copy()\n",
    "    im_new = change_rgb(im_new, new_values)\n",
    "    \n",
    "    return im_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import match_histograms\n",
    "\n",
    "def envelope_to_histogram(sample, scale, plot=False):\n",
    "    \"\"\"\n",
    "    takes sound sample and scale which defines the sum of all bins in the histogram.\n",
    "    In an image this would be heigt*width = number of pixels\n",
    "    scale has to be <= the number of pixels in the image channel\n",
    "    \n",
    "    returns histogram normalized that would correspond to an image with \"scale\" pixels\n",
    "    \"\"\"\n",
    "    num_bins = 256 # one bin for each brightness level in an image\n",
    "    bin_size = int(len(sample)/num_bins) # the amount of time steps that are summarized in one bin\n",
    "    if plot:\n",
    "        print(num_bins, \"with size\", bin_size)\n",
    "    binned_envelope = np.array([np.mean(sample[i:i+bin_size]) for i in range(num_bins)])\n",
    "    \n",
    "    # if a sample has no sound we add a small value to each bin\n",
    "    if binned_envelope.sum() < 0.1:\n",
    "        binned_envelope += 1\n",
    "    # normalize bins to sum=scale\n",
    "    normalized_float = (binned_envelope/binned_envelope.sum()) * scale\n",
    "    normalized = normalized_float.astype(int)\n",
    "    # make sure that the sum is correct after conversion to integer\n",
    "    while normalized.sum() != scale:\n",
    "        normalized_float = normalized_float % 1 # get only digits after comma\n",
    "        idx_biggest_number = np.argmax(normalized_float)\n",
    "        normalized[np.argmax(normalized_float)] += 1\n",
    "        normalized_float[idx_biggest_number] = 0\n",
    "    return normalized\n",
    "\n",
    "def get_reference_image(im_channel, diff_sound_hist):\n",
    "    \"\"\"\n",
    "    @param im_channel the image channel we want to change\n",
    "    @param diff_sound_hist the difference between two sound histograms according to which we want to modify the channel\n",
    "    get difference of two sound histograms add to initial channel and compute reference image for the channel\n",
    "    the reference image is an image with the desired histogram but not the image content \n",
    "    it can be used for histogram mapping with scipys match_histograms function\n",
    "    \"\"\"\n",
    "    h,w = im_channel.shape\n",
    "    \n",
    "    # histogram of initial color channel\n",
    "    idx, counts = np.unique(im_channel, return_counts = True)\n",
    "    hist_im = np.zeros(256, dtype=int)\n",
    "    hist_im[idx] = counts\n",
    "    \n",
    "    # add difference to image histogram\n",
    "    y_ax = hist_im + diff_sound_hist\n",
    "    \n",
    "    # perform all this normalization to have the correct pixel amount\n",
    "    y_ax[y_ax<0]=0 # make positive\n",
    "    y_ax_float = (y_ax/y_ax.sum())*w*h # normalize again to w*h to have the correct pixel amount\n",
    "    y_ax = y_ax_float.astype(int)\n",
    "    while y_ax.sum() != w*h:\n",
    "        y_ax_float = y_ax_float % 1 # get only digits after comma\n",
    "        idx_biggest_number = np.argmax(y_ax_float)\n",
    "        y_ax[idx_biggest_number] += 1\n",
    "        y_ax_float[idx_biggest_number] = 0\n",
    "    \n",
    "    # check that pixel amount is correct\n",
    "    assert(h*w==y_ax.sum())\n",
    "    \n",
    "    # build an image with the desired pixel histogram, the position of each pixel does not matter\n",
    "    im_ref = np.zeros((h,w), dtype=np.uint8)\n",
    "    intensity_bar = 0 # [0,255] which bar we are in right now\n",
    "    count = 0 # keep track of height of bar\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            while count == y_ax[intensity_bar]:\n",
    "                intensity_bar += 1\n",
    "                count = 0\n",
    "            im_ref[i,j] = intensity_bar\n",
    "            count += 1\n",
    "            \n",
    "    return im_ref\n",
    "\n",
    "def get_sub_samples(sample, num=3):\n",
    "    \"\"\"\n",
    "    function that devides a sound sample into three equal sized sound samples (one for each color channel)\n",
    "    \"\"\"\n",
    "    subsample_len = int(len(sample)/num)\n",
    "    return sample[:subsample_len], sample[subsample_len:subsample_len*2], sample[subsample_len*2:subsample_len*3]\n",
    "\n",
    "def image_from_histograms(sample1, sample2, im, scale):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # cut into three subsamples and transform envelopes to histograms\n",
    "    r1, g1, b1 = (envelope_to_histogram(subsample, scale) for subsample in get_sub_samples(sample1))\n",
    "    r2, g2, b2 = (envelope_to_histogram(subsample, scale) for subsample in get_sub_samples(sample2))\n",
    "    \n",
    "    # reference image in which we store the manipulated color channels\n",
    "    ref_im = np.zeros(im.shape)\n",
    "    \n",
    "    # calculate the differences between the three channels\n",
    "    diff_red = r2-r1\n",
    "    diff_blue = b2-b1\n",
    "    diff_green = g2-g1\n",
    "    #print(f\"sum should be scale ={scale}: {r1.sum()==scale}, {r2.sum()==scale}, {b1.sum()==scale}, {b2.sum()==scale}, {g1.sum()==scale}, {g2.sum()==scale}\")\n",
    "    #print(len(diff_red))\n",
    "    \n",
    "    # get reference image for each channel\n",
    "    ref_im[:,:,0] = get_reference_image(im[:,:,0], diff_red)\n",
    "    ref_im[:,:,1] = get_reference_image(im[:,:,1], diff_green)\n",
    "    ref_im[:,:,2] = get_reference_image(im[:,:,2], diff_blue)\n",
    "    \n",
    "    # do the actual matching\n",
    "    new_im = match_histograms(im, ref_im, multichannel=True)\n",
    "    return new_im\n",
    "\n",
    "def match_histograms_from_envelope(features1, features2, img):\n",
    "    # keep the same scale as before\n",
    "    h,w,d = img.shape # image height and with to know the sum of the bins of the image histogram\n",
    "    scale = int(h*w)\n",
    "\n",
    "    sample1 = features1[\"values\"] #audio_pos[sample_len*(i-1):i*sample_len]\n",
    "    sample1[sample1<0] = 0\n",
    "    # use only positive part\n",
    "    sample2 = features2[\"values\"]\n",
    "    sample2[sample2<0] = 0\n",
    "    \n",
    "    if len(sample1) <= 256*3:\n",
    "        print(\"reference sample is too short\")\n",
    "        return img\n",
    "    if len(sample2) <= 256*3:\n",
    "        print(\"new sample is too short\")\n",
    "        return img\n",
    "    \n",
    "    new_im = image_from_histograms(sample1, sample2, img, scale)\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that should be different for other approaches of mapping\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    new_img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return new_img\n",
    "\n",
    "def decrease_brightness(img, value=-30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = -value\n",
    "    v[v < lim] = 0\n",
    "    v[v >= lim] -= (-value)\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    new_img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return new_img\n",
    "\n",
    "def change_brightness(img, value=0):\n",
    "    if value == 0:\n",
    "        return img\n",
    "    if value < 0:\n",
    "        return decrease_brightness(img, value)\n",
    "    return increase_brightness(img, value)\n",
    "\n",
    "def change_brightness2(img, brightness_offset):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = np.clip(v + brightness_offset, 0, 255).astype('uint8')\n",
    "    return cv2.cvtColor(cv2.merge((h, s, v)), cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def change_rgb(img, target_values):\n",
    "    \"\"\"changes rgb values in place to target_values\"\"\"\n",
    "    # current BGR values of image\n",
    "    RGB = np.mean(np.mean(img, axis = 1), axis=0)\n",
    "    difference = target_values - RGB\n",
    "    for i in range(3):\n",
    "        # do not change if target value is 0 bc that means that the feature is not relevant\n",
    "        if target_values[i] == 0:\n",
    "            continue\n",
    "        # difference image for cv2.subtract/add\n",
    "        diff = np.zeros_like(img)\n",
    "        # fill with the magnitude of the difference between the means\n",
    "        diff[:,:,i] = abs(difference[i])\n",
    "        # if difference is negative we need to decrease the mean of the channel i\n",
    "        if difference[i] < 0:\n",
    "            img = cv2.subtract(img, diff)\n",
    "        # if positive we need to increase it\n",
    "        else:\n",
    "            img = cv2.add(img, diff)\n",
    "    # write the resulting image back to the file\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing shapes\n",
    "\n",
    "Input: frequency, amplitude, time + hyperparameters\n",
    "\n",
    "Parameters:\n",
    "- angle within spiral: frequency, time\n",
    "- distance from centre: time, freqency\n",
    "- oscillation magnitude: frequency\n",
    "- oscillation frequency: frequency\n",
    "- alpha: amplitude, frequency\n",
    "- radius: amplitude, frequency\n",
    "- color: frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_shapes(img, features, t, sr):\n",
    "    img = img.copy()\n",
    "    h, w, _ = img.shape\n",
    "    db_threshold = 8\n",
    "    center_x, center_y = w//2, h//2\n",
    "    \n",
    "    frequencies = librosa.fft_frequencies(sr=sr, n_fft=features['param_n_fft'])\n",
    "    freq_min, freq_max = 30, 15000 # minimum and maximum frequencies to draw\n",
    "    freq_indices = (frequencies >= freq_min) & (frequencies <= freq_max) # indices of frequencies to draw\n",
    "    frequencies = frequencies[freq_indices]\n",
    "    \n",
    "    for i, val in enumerate(features['stft_db'][freq_indices]):\n",
    "        shapes = np.zeros_like(img, np.uint8)\n",
    "        mask = np.zeros_like(img, np.uint8)\n",
    "    \n",
    "        magnitude = val - db_threshold\n",
    "        if magnitude <= 0:\n",
    "            continue\n",
    "        freq = frequencies[i]\n",
    "        rel_frequency = (np.log(freq) - np.log(freq_min)) / (np.log(freq_max) - np.log(freq_min)) # a number between 0 (=lowest frequency) and 1 (=highest frequency)\n",
    "        def spiral(x):\n",
    "            x = np.interp(x, [0, 0.2, 1.], [0., 0.1, 1.])\n",
    "            magnitude = x ** 0.5 * 0.6\n",
    "            magnitude_offset = 0.1 * np.interp(x, [0, 1], [0.2, 1]) * np.sin((2*np.pi*t + 1000) * np.interp(x, [0,1], [0.5, 4]))\n",
    "            magnitude += magnitude_offset\n",
    "            angle = 2*np.pi* (2*x - t*0.6)\n",
    "            point = (\n",
    "                center_x + int( magnitude * np.cos(angle) * min(w,h) ),\n",
    "                center_y + int( magnitude * np.sin(angle) * min(w,h) )\n",
    "            )\n",
    "            return point\n",
    "        intensity = np.log(magnitude+1) * np.interp(rel_frequency, [0,1], [1., 0.2])\n",
    "        center = spiral(rel_frequency)\n",
    "        radius = int(intensity * min(h, w) * 0.03)\n",
    "        rgb = np.array(cm.hsv(1 - rel_frequency))*255\n",
    "        alpha = np.clip(\n",
    "            intensity * 0.35,\n",
    "            0, 1)\n",
    "        \n",
    "        #cv2.circle(img, center, radius, rgba, cv2.FILLED)\n",
    "        cv2.circle(shapes, center, radius, rgb, cv2.FILLED)\n",
    "        cv2.circle(mask, center, radius, (255,255,255), cv2.FILLED)\n",
    "    \n",
    "        # Generate output by blending image with shapes image, using the mask\n",
    "        mask = mask.astype(bool)\n",
    "        img[mask] = cv2.addWeighted(img, alpha, shapes, alpha, 0)[mask]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_image_frame(source_img, features, t, sr, features_initial=0):\n",
    "#     img = source_img\n",
    "\n",
    "#     #img = match_histograms_from_envelope(features_initial, features, img)\n",
    "#     #img = sound_to_image(img, features, features_initial, sr, mode=\"maximum\")\n",
    "#     img = change_brightness2(img, (features['amplitude_abs_mean'] - 0.5) * 1 * 255)\n",
    "#     img = draw_shapes(img, features, t, sr)\n",
    "    \n",
    "#     return img\n",
    "\n",
    "def get_image_sequence(source_img, frames, sr, f, callback=None):\n",
    "    def process_frame(i, frame):\n",
    "        result = get_image_frame(source_img, frame, i/(len(frames)-1), sr, frames[len(frames)//2])\n",
    "        if callback:\n",
    "            callback(i)\n",
    "        return result\n",
    "    return [ process_frame(i, frame) for i, frame in enumerate(frames) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_video(imgs, fps, video_name, target_width=480):\n",
    "    height, width, layers = imgs[0].shape\n",
    "    video = cv2.VideoWriter(video_name, 0, fps, (width, height))\n",
    "    for image in imgs:\n",
    "        video.write(cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "def merge_video_audio(video_name, fps, audio_name, video_out_name):\n",
    "    # load the video\n",
    "    video_clip = VideoFileClip(video_name)\n",
    "    # load the audio\n",
    "    audio_clip = AudioFileClip(audio_name)\n",
    "    common_end = min(video_clip.end, audio_clip.end) # ends should be the same, just to be sure\n",
    "    video_clip = video_clip.subclip(0, common_end)\n",
    "    audio_clip = audio_clip.subclip(0, common_end)\n",
    "    final_clip = video_clip.set_audio(audio_clip)\n",
    "    final_clip.write_videofile(video_out_name, fps=fps)\n",
    "    final_clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy/Onset differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rhythm(song):\n",
    "    \"\"\"\n",
    "    Outputs a list of timestamps, in seconds, for which \n",
    "    onsets were detected by using the default onset \n",
    "    detection function. \n",
    "    \"\"\"\n",
    "    # Onsets\n",
    "    onsets_string = subprocess.check_output(['aubioonset', '-i', song])\n",
    "    onsets = onsets_string.decode(\"utf-8\").strip().split('\\n')\n",
    "    onsets = [float(onset) for onset in onsets]\n",
    "    # Diffs\n",
    "    # Gives us a list of timings between onsets\n",
    "    diffs = np.abs(np.diff(onsets)) # Calculate the n-th discrete difference along the given axis\n",
    "    grid = np.linspace(0, 10, 1000)\n",
    "    density = scipy.stats.gaussian_kde(diffs)\n",
    "    hist = density(grid)\n",
    "    # Entropy\n",
    "    entropy = scipy.stats.entropy(hist) # Shannon's entropy \n",
    "\n",
    "    return {\n",
    "    'entropy': entropy,\n",
    "    'recording': song,\n",
    "    'onsets': onsets,\n",
    "    'diffs': diffs,\n",
    "    'hist': hist,\n",
    "    'grid': grid\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateOnsetDiffs(values):\n",
    "    print('od')\n",
    "\n",
    "    onsets = values['onsets']\n",
    "    diffs = values['diffs']\n",
    "\n",
    "    plt.title('Onset Diffs')\n",
    "    plt.ylabel('Onset Time Difference (s)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    graphDiffs = np.append(diffs, [0])\n",
    "    plt.plot(onsets, graphDiffs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSmoothedOnsetHistogram(values,ax):\n",
    "    \"\"\"\n",
    "    Corrects the statistical errors in the observation\n",
    "    of the onsets. We use Gaussian kernels as a specific \n",
    "    shape in estimating the actual data's shape (since we\n",
    "    assume the musicians' mistakes are normally distributed).\n",
    "    \"\"\"\n",
    "    print('soh')\n",
    "\n",
    "    diffs = values['diffs']\n",
    "    hist = values['hist']\n",
    "    grid = values['grid']\n",
    "\n",
    "    ax.set_title('Smoothed Onset Diff Histogram')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xlabel('Onset Time Difference (s)')\n",
    "    bins = ax.hist(diffs, bins=300, density=True)\n",
    "    histNorm = hist / np.max(hist) * np.max(bins[0])\n",
    "    end = [i for i, v in enumerate(hist) if v > 1e-4][-1]\n",
    "    ax.plot(grid[:end], histNorm[:end], 'g-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'aubioonset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/3226573785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_rhythm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_rhythm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"harry_styles.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/4233356376.py\u001b[0m in \u001b[0;36mcompute_rhythm\u001b[0;34m(song)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Onsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0monsets_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aubioonset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msong\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0monsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monsets_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0monsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0monset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0monsets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    425\u001b[0m                **kwargs).stdout\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aubioonset'"
     ]
    }
   ],
   "source": [
    "res1 = compute_rhythm(audio_name)\n",
    "res2 = compute_rhythm(p_join(data_fpath, \"harry_styles.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/2853073510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerateOnsetDiffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res1' is not defined"
     ]
    }
   ],
   "source": [
    "generateOnsetDiffs(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/3055962794.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtight_layout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgenerateSmoothedOnsetHistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgenerateSmoothedOnsetHistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res1' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,2, tight_layout = True, figsize = (12,5))\n",
    "axes.flatten()\n",
    "generateSmoothedOnsetHistogram(res1, axes[0])\n",
    "generateSmoothedOnsetHistogram(res2, axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue bars of the graph represent the onset difference while the green line represents the Parzen smoothed probability density function. So, the density of onset differences in the song.\n",
    "We can see on the left-most plot, the onset time difference is mainly around 0.22 s. In other words, most onsets occurring in the song appear at an interval of 0.22s. Whereas, with \"Watermelon Sugar\" from Harry Styles, we've got more spread out onset time differences, which would underline a higher entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/377791246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res1' is not defined"
     ]
    }
   ],
   "source": [
    "print(res1['entropy'])\n",
    "print(res2['entropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the entropy is greater for the Harry Styles song (number 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes audio info and image as an imput an returns set of images for the frame\n",
    "def get_gif(info, img_address):\n",
    "    img = cv2.imread(img_address)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    ln = len(info)\n",
    "    amplitudes = []\n",
    "    for i in range(ln):\n",
    "        amplitudes.append(info[i]['amplitude'].max())\n",
    "    br_values = np.array(amplitudes) / min(amplitudes)\n",
    "    # this arguments just hand picked, so the images looks different\n",
    "    # for final version better use proper transformation\n",
    "    a = 19.14\n",
    "    b = -64.12\n",
    "    br_values = br_values * a + b\n",
    "    br_values = np.array(br_values, dtype=int)\n",
    "    new_imgs = []\n",
    "    for v in br_values:\n",
    "        new_imgs.append(change_brightness(img, v))\n",
    "    return new_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/944157970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoin\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp_join\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mPROJDIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnew_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJDIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data/apple.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'frames' is not defined"
     ]
    }
   ],
   "source": [
    "from os.path import abspath\n",
    "from os.path import join as p_join\n",
    "PROJDIR = abspath('')\n",
    "new_imgs = get_gif(frames, p_join(PROJDIR,'data/apple.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/2614480009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# basic visualization of images in result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_imgs' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# basic visualization of images in result\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i in range(1,len(new_imgs)):\n",
    "    fig.add_subplot(5,6,i)\n",
    "    plt.imshow(new_imgs[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/4180033676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'out.gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mnew_img\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_imgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "# this function is for making gif, the problem I didn't find an argument on how to change the time for particular image\n",
    "# so, it will need additional coding\n",
    "import imageio\n",
    "with imageio.get_writer(f'out.gif', mode='I') as writer:\n",
    "    for new_img in new_imgs:\n",
    "        writer.append_data(new_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, here we should collect the similarity metrics vector. \n",
    "Approximately, it should look like that:\n",
    "```\n",
    "def img_metric(imgs):\n",
    "    vec = []\n",
    "    for i in range(len(imgs) - 1):\n",
    "        vec.append(rmse(imgs[i], imgs[i + 1]))\n",
    "    return vec\n",
    "    \n",
    "def audio_metric(frames):\n",
    "    ...\n",
    "    return some_vec\n",
    "\n",
    "audio_vec = normalize(audio_metric...)\n",
    "img_vec = normalize(img_metric...)\n",
    "Goal_metric = cosine_distance(audio_vec, img_vec)\n",
    "```\n",
    "\n",
    "The goal is to make the best goal metric, which will be equal to similarity between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'compare_mse' from 'skimage.measure' (/opt/anaconda3/lib/python3.9/site-packages/skimage/measure/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/989240621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# additional libraries for metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare_mse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'compare_mse' from 'skimage.measure' (/opt/anaconda3/lib/python3.9/site-packages/skimage/measure/__init__.py)"
     ]
    }
   ],
   "source": [
    "# additional libraries for metrics\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "from skimage.measure import compare_mse as mse\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(audio_name, window_size):\n",
    "    # audio_name -- name of audiofile\n",
    "    # window_size -- we split audio into frames, this is the length of the frame in seconds\n",
    "    x, sr = librosa.load(audio_name)\n",
    "    seconds = len(x) / sr\n",
    "    frame_len = int(len(x) / (seconds / window_size))\n",
    "    frames = []\n",
    "    for ind in range(0, len(x) + 1, frame_len):\n",
    "        current_amplitude = x[ind:ind + frame_len]\n",
    "        fft = librosa.stft(current_amplitude, n_fft=len(current_amplitude), hop_length=len(current_amplitude))\n",
    "        freqs = fft[:, 0]\n",
    "        freqs = librosa.amplitude_to_db(abs(freqs))\n",
    "        _, phase = librosa.magphase(fft)\n",
    "        phase = phase[:, 0]\n",
    "        frames.append({'amplitude':current_amplitude, 'frequency':freqs, 'phase':phase})\n",
    "        #timbre = 0\n",
    "    return frames\n",
    "audio_name = 'data/snd.wav'\n",
    "frames = get_info(audio_name, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_imgs = get_gif(frames, 'data/apple.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/2399800502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the smaller the better since (1 - normed dot product)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/2399800502.py\u001b[0m in \u001b[0;36mcosine_distance\u001b[0;34m(imgs, frames)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimg_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0maudio_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/7q/bg95bl3j53l73yx_zwh45xxr0000gn/T/ipykernel_21382/2399800502.py\u001b[0m in \u001b[0;36mimg_metric\u001b[0;34m(imgs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mse' is not defined"
     ]
    }
   ],
   "source": [
    "def img_metric(imgs):\n",
    "    vec = []\n",
    "    for i in range(len(imgs) - 1):\n",
    "        vec.append(np.sqrt(mse(imgs[i], imgs[i + 1])))\n",
    "    return np.array(vec)\n",
    "\n",
    "def audio_metric(frames):\n",
    "    vec = []\n",
    "    for i in tqdm(range(len(frames) - 1)):\n",
    "        distance, _ = fastdtw(frames[i]['amplitude'], frames[i + 1]['amplitude'], dist=euclidean)\n",
    "        vec.append(distance)\n",
    "    return np.array(vec)\n",
    "\n",
    "def cosine_distance(imgs, frames):\n",
    "    img_vec = img_metric(new_imgs)\n",
    "    audio_vec = audio_metric(frames)\n",
    "    return cosine(img_vec, audio_vec)\n",
    "\n",
    "print(cosine_distance(new_imgs, frames)) # the smaller the better since (1 - normed dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>1. Choose an audio file</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541ca989aa3d4e2ebbab1a5639eaf4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Audio file:', options=('data/Patti Austin - Thats Enough For Me.wa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>2. Choose an image</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2482710146714e60af9cfc8434144687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', options=('data/apple.jpg', 'data/apples.jpg'), value='dat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>3. Choose effects</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e61dc52b5e429888c16ee49f5fad46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='Selected effects', options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedabb1b8bf042328e0239bf47738181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Remove selected', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr style=\"width: 350px; margin-left: 0;\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4baa1a727e74a76868878a522700ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Effect:', options=(('(No-op)', 'no-op'), ('Amplitude envelope -> B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0629f2896a449d8100021b79ffcccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Add effect', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>4. Have fun!</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd91de0ce154fba8e562d1336b325c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate a video', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd101c15e4014ad691445b4f697e21de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>5. See the metric</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.html.widgets import interactive\n",
    "\n",
    "def display_h2(text):\n",
    "    display(HTML('<h2>{}</h2>'.format(text)))\n",
    "\n",
    "#--- Choose an audio file ---\n",
    "selected_audio = ([], 0, \"\")\n",
    "def display_audio(audio_file):\n",
    "    global selected_audio\n",
    "    audio, sr = librosa.load(audio_file, sr=None)\n",
    "    if len(audio.shape) > 1: # If stereo, use only the left channel\n",
    "        audio = audio.T[0]\n",
    "    audio = audio[0:10*sr] # Keep max 10s of the audio\n",
    "    selected_audio = audio, sr, audio_file\n",
    "    \n",
    "    print(audio_file)\n",
    "    display(Audio(audio, rate=sr))\n",
    "    plt.plot(audio)\n",
    "    plt.show()\n",
    "\n",
    "display_h2(\"1. Choose an audio file\")\n",
    "dropdown_audio_file = widgets.Dropdown(options=sorted(glob.glob('data/*.wav')), description=\"Audio file:\")\n",
    "display(interactive(display_audio, audio_file=dropdown_audio_file))\n",
    "\n",
    "#--- Choose an image ---\n",
    "selected_image = np.array([])\n",
    "def select_image(image_file, width):\n",
    "    global selected_image\n",
    "    img = cv2.imread(image_file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = resize_image(img, target_width=width)\n",
    "    selected_image = img\n",
    "    \n",
    "    print(\"{} ({}x{})\".format(image_file, *img.shape))\n",
    "    plt.imshow(img)\n",
    "\n",
    "display_h2(\"2. Choose an image\")\n",
    "dropdown_image_file = widgets.Dropdown(options=sorted(glob.glob('data/*.jpg')), description=\"Image:\")\n",
    "inttext_image_width = widgets.BoundedIntText(value=480, min=1, max=4096, step=1, description='Target width (px):')\n",
    "display(interactive(select_image, image_file=dropdown_image_file, width=inttext_image_width))\n",
    "\n",
    "#--- Selected effects ---\n",
    "display_h2(\"3. Choose effects\")\n",
    "selected_effects = widgets.Select(description=\"Selected effects\", options=[])\n",
    "display(selected_effects)\n",
    "\n",
    "button_remove_selected_effects = widgets.Button(description=\"Remove selected\")\n",
    "def on_button_remove_selected_effects_click(b):\n",
    "    options = list(selected_effects.options)\n",
    "    del options[selected_effects.index]\n",
    "    selected_effects.options = options\n",
    "button_remove_selected_effects.on_click(on_button_remove_selected_effects_click)\n",
    "display(button_remove_selected_effects)\n",
    "\n",
    "display(HTML(\"\"\"<hr style=\"width: 350px; margin-left: 0;\"/>\"\"\"))\n",
    "\n",
    "#--- Add an effect ---\n",
    "selected_effect = {}\n",
    "def select_effect(effect, **kwargs):\n",
    "    def accept_params(**params):\n",
    "        global selected_effect\n",
    "        selected_effect = {\n",
    "            \"effect\": effect,\n",
    "            **params\n",
    "        }\n",
    "    return accept_params\n",
    "\n",
    "def effect_parameters(effect):\n",
    "    params = {}\n",
    "    if effect == 'amplitude_brightness':\n",
    "        params['intensity'] = widgets.FloatSlider(\n",
    "            value=0.5,\n",
    "            min=0,\n",
    "            max=1.0,\n",
    "            step=0.01,\n",
    "            description='Intensity:',\n",
    "            readout_format='.2f',\n",
    "        )\n",
    "    display(interactive(select_effect(effect), **params))\n",
    "\n",
    "dropdown_effect = widgets.Dropdown(description=\"Effect:\", options=[\n",
    "    ('(No-op)', 'no-op'),\n",
    "    ('Amplitude envelope -> Brightness', 'amplitude_brightness'),\n",
    "    ('Match histogram from amplitude envelope', 'match_histograms_from_envelope'),\n",
    "    ('Low/Mid/High freq. amplitude -> R/G/B channels intensity', 'frequency_bins'),\n",
    "    ('Draw shapes', 'draw_shapes'),\n",
    "])\n",
    "display(interactive(effect_parameters, effect=dropdown_effect))\n",
    "\n",
    "button_add_effect = widgets.Button(description=\"Add effect\")\n",
    "def on_button_add_effect_click(b):\n",
    "    global selected_effects\n",
    "    selected_effects.options = list(selected_effects.options) + [selected_effect]\n",
    "    #selected_effects.options = list(selected_effects.options) + [\"a\"]\n",
    "button_add_effect.on_click(on_button_add_effect_click)\n",
    "display(button_add_effect)\n",
    "\n",
    "#--- Applying selected effects ---\n",
    "def apply_effect(img, features, t, sr, features_initial, effect):\n",
    "    if effect['effect'] == 'amplitude_brightness':\n",
    "        return change_brightness2(img, (features['amplitude_abs_mean'] - 0.5) * 2 * 255 * effect['intensity'])\n",
    "    if effect['effect'] == 'match_histograms_from_envelope':\n",
    "        return match_histograms_from_envelope(features_initial, features, img)\n",
    "    if effect['effect'] == 'frequency_bins':\n",
    "        return sound_to_image(img, features, features_initial, sr, mode=\"maximum\")\n",
    "    if effect['effect'] == \"draw_shapes\":\n",
    "        return draw_shapes(img, features, t, sr)\n",
    "    return img\n",
    "        \n",
    "def get_image_frame(img, features, t, sr, features_initial=0):\n",
    "    for effect in selected_effects.options:\n",
    "        img = apply_effect(img, features, t, sr, features_initial, effect)\n",
    "    return img\n",
    "\n",
    "#--- Generate the video ---\n",
    "display_h2(\"4. Have fun!\")\n",
    "button_generate_video = widgets.Button(description=\"Generate a video\")\n",
    "output_generate_video = widgets.Output()\n",
    "def on_button_generate_video_click(b):\n",
    "    global output_generate_video\n",
    "    with output_generate_video:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        audio, sr, audio_name = selected_audio\n",
    "        img = selected_image\n",
    "\n",
    "        # Calculate features\n",
    "        print(\"Analyzing the audio file...\")\n",
    "        frames = get_features(audio, sr, 0.1)\n",
    "\n",
    "        # apply effects to each frame\n",
    "        print(\"Generating image frames...\")\n",
    "        frames_progress = widgets.IntProgress(min=0, max=len(frames))\n",
    "        display(frames_progress)\n",
    "        def frames_progress_callback(k):\n",
    "            frames_progress.value = k\n",
    "        imgs = get_image_sequence(img, frames, sr, get_image_frame, frames_progress_callback)\n",
    "        frames_progress.close()\n",
    "\n",
    "        # generate the video\n",
    "        video_name = 'output_temp.avi' # intermediate file without audio\n",
    "\n",
    "        duration_s = len(audio) / sr\n",
    "        fps = len(imgs) / duration_s\n",
    "        generate_video(imgs, fps, video_name)\n",
    "\n",
    "        # merge the video with the original audio\n",
    "        video_out_name = \"output.mp4\"\n",
    "        merge_video_audio(video_name, fps, audio_name, video_out_name)\n",
    "\n",
    "        # display the video\n",
    "        display(HTML(\"\"\"<video width=\"400\" controls><source src=\"{}\"></video>\"\"\".format(video_out_name)))\n",
    "\n",
    "button_generate_video.on_click(on_button_generate_video_click)\n",
    "display(button_generate_video, output_generate_video)\n",
    "\n",
    "#--- Metric ---\n",
    "display_h2(\"5. See the metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
